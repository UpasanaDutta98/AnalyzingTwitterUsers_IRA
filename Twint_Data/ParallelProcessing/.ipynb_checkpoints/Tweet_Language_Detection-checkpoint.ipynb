{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langdetect import DetectorFactory\n",
    "DetectorFactory.seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4641\n",
      "Number of usernames to scrape before bot filtering =  4641\n",
      "Number of bots =  182\n",
      "Removed users ------\n",
      "Number of usernames to scrape after bot filtering =  4641\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "Allfilenames=[]\n",
    "mentioned_df = pd.read_csv(\"Famous_Users/didNotMentionBack_users.csv\")\n",
    "Allfilenames= list(mentioned_df['DidnotMentionBack Usernames'])\n",
    "print(len(Allfilenames))\n",
    "\n",
    "# Now remove bots\n",
    "print(\"Number of usernames to scrape before bot filtering = \", len(Allfilenames))\n",
    "file2 = open(\"Famous_Users/botometer_removed_users.txt\",\"r+\")  \n",
    "bot_List = file2.readlines()\n",
    "print(\"Number of bots = \", len(bot_List))\n",
    "print(\"Removed users ------\")\n",
    "for botname in bot_List:\n",
    "    botname = botname.strip()\n",
    "    #full_name = name+\"_tweet_only.csv\"\n",
    "    #print(botname)\n",
    "    if botname in Allfilenames:\n",
    "        print(botname)\n",
    "        Allfilenames.remove(botname)\n",
    "\n",
    "print(\"Number of usernames to scrape after bot filtering = \", len(Allfilenames))\n",
    "#print(read_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this are some bots because in the first run done inside the democracy project, I hadnt removed the bots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['creynoldsnc',\n",
       " 'BuzzFeedVideo',\n",
       " 'iowahawkblog',\n",
       " 'GmoneyRainmaker',\n",
       " 'komandnaya',\n",
       " 'Nettaaaaaaaa',\n",
       " 'NWOinPanicMode',\n",
       " 'CAWPBT',\n",
       " 'sashakots',\n",
       " 'Conservatexian']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Allfilenames[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import multiprocessing\n",
    "from urlextract import URLExtract\n",
    "import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import preprocessor.api as p\n",
    "\n",
    "def remove_URLs_emojis_hashtags_handles_numbers(tweet):\n",
    "    if type(tweet) != str:\n",
    "        return str(tweet)\n",
    "    #print(tweet)\n",
    "    \n",
    "    # first remove URLs because p.clean sometimes does not work for certain kinds of URLs.\n",
    "    \n",
    "    #for each_url in urlList:\n",
    "     #   tweet = tweet.replace(each_url, '')\n",
    "        \n",
    "    extractor = URLExtract()\n",
    "    urls = extractor.find_urls(tweet)\n",
    "    for url in urls:\n",
    "        tweet = tweet.replace(url, '')\n",
    "        \n",
    "        \n",
    "    return p.clean(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_symbols(tweet):\n",
    "    if type(tweet) != str:\n",
    "        return str(tweet)\n",
    "    symbols = \"`~!@#$%^&*()-_=+[{}]\\|;:'\\\",<.>/?\"\n",
    "    for symbol in symbols:\n",
    "        tweet = tweet.replace(symbol, '')\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urlextract import URLExtract\n",
    "\n",
    "def remove_URLs(tweet):\n",
    "    if type(tweet) != str: #This is for a tweet in our tweet CSVs that has been scraped as a float, so we get error\n",
    "        return str(tweet)\n",
    "    \n",
    "    try:\n",
    "        extractor = URLExtract()\n",
    "        urls = extractor.find_urls(tweet)\n",
    "        for url in urls:\n",
    "            tweet = tweet.replace(url, '')\n",
    "    except Exception as e:\n",
    "        print(repr(e))\n",
    "        pass\n",
    "        \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints\n",
      "2434\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cleaned_users = []\n",
    "for file in os.listdir(\"Famous_Users/User_Data/English_Only_Tweets\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        cleaned_users.append(file[:-4])\n",
    "    else:\n",
    "        print(file)\n",
    "print(len(cleaned_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eshaLegal'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_users[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_language_detection_within_range(starting_range, ending_range, return_dict):\n",
    "    countdone = 0\n",
    "    print(\"This is for users in range - \", starting_range, ending_range)\n",
    "    user_language_count = {}\n",
    "    finaltime1 = time.time()\n",
    "    number_of_tweets_scanned = 0\n",
    "    number_of_tweets_with_foreignLang = 0 ## this is out of the tweets whose language could be detected.\n",
    "    number_of_tweets_with_NoLang = 0\n",
    "    fraction_of_englishTweets = []\n",
    "    exceptions_received = set()\n",
    "    tweets_with_errors = []\n",
    "    for k in range(starting_range, ending_range):                ## Lets run for the first 1500 users\n",
    "        if Allfilenames[k] in cleaned_users:\n",
    "            countdone += 1\n",
    "            continue\n",
    "        time1 = time.time()\n",
    "        tweets_df = pd.read_csv(\"Famous_Users/User_Data/tweetOnly_files/\"+ Allfilenames[k]+'_tweet_only.csv', engine = 'c',low_memory=False)\n",
    "        all_tweets = tweets_df['tweet']\n",
    "        #urls = tweets_df['urls']\n",
    "        language_count_dict = defaultdict(int)  ## We store the total language frequencies for every user\n",
    "        total_DetectedTweetcount_forUser = 0\n",
    "        englishCount_forUser = 0\n",
    "        length = len(all_tweets)\n",
    "        \n",
    "        \n",
    "        sentence = \"Starting user - \" + str(Allfilenames[k]) + \"with\" + str(length) + \"tweets at\" + str(datetime.datetime.now()) + \"\\n\"\n",
    "        #print(sentence)\n",
    "        file_object = open('TimeTakenforLanguageDetection.txt', 'a')\n",
    "        file_object.write(sentence)\n",
    "        # Close the file\n",
    "        file_object.close()\n",
    "        with open('Famous_Users/User_Data/CleanedTweets/' + str(Allfilenames[k]+\".csv\"), 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            with open('Famous_Users/User_Data/English_Only_Tweets/' + str(Allfilenames[k]+\".csv\"), 'w', newline='') as file2:\n",
    "                writer2 = csv.writer(file2)\n",
    "                for i in range(length):\n",
    "                    #if i%5000 == 0:\n",
    "                    #    print(\"reached \", i, \"for user\", Filtered_filenames[k])\n",
    "                    number_of_tweets_scanned += 1\n",
    "\n",
    "                    this_tweet = all_tweets[i]\n",
    "                    try:\n",
    "        #                 urlList = []\n",
    "        #                 urlList_string = urls[i]\n",
    "        #                 if len(urlList_string) > 2:   # i.e the string is bigger than just '[]'\n",
    "        #                     urlList = urlList_string.split(', ')\n",
    "        #                     urlList[0] = urlList[0][1:]\n",
    "        #                     urlList[-1] = urlList[-1][:-1]\n",
    "\n",
    "                        ### Cleaning the tweet\n",
    "                        new_tweet = remove_URLs_emojis_hashtags_handles_numbers(this_tweet)\n",
    "\n",
    "\n",
    "\n",
    "                        ### Now removing symbols from tweet\n",
    "                        #new_tweet = this_tweet\n",
    "                        tweet = remove_symbols(new_tweet)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        # if the tweet cannot be cleaned for some error, we try working with our function to remove just the URLs\n",
    "                        print(type(this_tweet), this_tweet)\n",
    "                        tweet = remove_URLs(this_tweet)\n",
    "\n",
    "\n",
    "                    ## Now save the cleaned tweet for this user\n",
    "                    row_to_list = list(tweets_df.iloc[i])\n",
    "                    row_to_list[10] = new_tweet # replace original tweet in the row with the cleaned tweet\n",
    "                    writer.writerow(row_to_list) # write it to the csv file\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    ### Now we try for language detection\n",
    "                    try :\n",
    "                        language = detect(tweet)\n",
    "                        language_count_dict[language] += 1 # for dictionary of all languages used by this user\n",
    "                        total_DetectedTweetcount_forUser += 1  # for count of all tweets whose language could be detected\n",
    "                        if language == 'en':\n",
    "                            englishCount_forUser += 1  # count of all english tweets of this user\n",
    "                            row_to_list = list(tweets_df.iloc[i])\n",
    "                            row_to_list[10] = new_tweet # replace original tweet in the row with the cleaned tweet but do not remove the symbols because they help in sentiment analysis\n",
    "                            writer2.writerow(row_to_list) # write it to the csv file\n",
    "                        else:\n",
    "                            number_of_tweets_with_foreignLang += 1  # count of all non-english tweets of this user\n",
    "\n",
    "                    except Exception as e:\n",
    "                        number_of_tweets_with_NoLang += 1  # total count of tweets whose language could not be detected\n",
    "                        exceptions_received.add(repr(e)) \n",
    "                        tweets_with_errors.append(tweet)\n",
    "\n",
    "        try:\n",
    "            fraction_of_englishTweets.append((Allfilenames[k], englishCount_forUser/total_DetectedTweetcount_forUser)) ## this is... out of all tweets whose language could be detected, what fraction of them were in english langiuage.\n",
    "\n",
    "        except:\n",
    "            print(\"DIVIDE BY ZERO ERROR! \")\n",
    "        user_language_count[Allfilenames[k]] = language_count_dict # key(username) -> value(their language count dictionary)\n",
    "        time2 = time.time()\n",
    "        sentence = \"User \" + str(Allfilenames[k]) + \" number \" + str(k) + str((starting_range, ending_range))+ \" , \" + str(all_tweets.shape[0]) + \" rows scanned in \" + str(time2-time1) + \" seconds at \" + str(datetime.datetime.now()) + \"\\n\"\n",
    "        file_object = open('TimeTakenforLanguageDetection.txt', 'a')\n",
    "        file_object.write(sentence)\n",
    "        # Close the file\n",
    "        file_object.close()\n",
    "        print(sentence)\n",
    "    returnList = [user_language_count, number_of_tweets_scanned, number_of_tweets_with_foreignLang, number_of_tweets_with_NoLang, fraction_of_englishTweets, exceptions_received, tweets_with_errors]\n",
    "    return_dict[(starting_range, ending_range)] = returnList\n",
    "    print(\"countdone = \", countdone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before running, first remove the users whose language analysis is done and are stored in ListsReturned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading all the saved pickle files\n",
    "import pickle\n",
    "pkl_file = open('Famous_Users/User_Data/ListsReturned/MentionedBack_'+str(1)+'.pkl', 'rb')\n",
    "returnedLists = pickle.load(pkl_file)\n",
    "\n",
    "for i in range(2,4):\n",
    "    pkl_file = open('Famous_Users/User_Data/ListsReturned/DidNotMentionBack_'+str(i)+'.pkl', 'rb')\n",
    "    List = pickle.load(pkl_file)\n",
    "    returnedLists = returnedLists + List\n",
    "    pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(returnedLists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1390"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#returnedLists = returnedLists# + returnedLists_2 #+ returnedLists_3 + returnedLists_4 + returnedLists_5\n",
    "number_of_processes = 46\n",
    "total_tweets_scanned = 0\n",
    "total_tweets_with_foreign_languages = 0\n",
    "total_tweets_with_no_languages = 0\n",
    "List_of_englishFractionTuples = []\n",
    "for i in range(number_of_processes):\n",
    "    total_tweets_scanned += returnedLists[i][1]\n",
    "    total_tweets_with_foreign_languages += returnedLists[i][2]\n",
    "    total_tweets_with_no_languages += returnedLists[i][3]\n",
    "    List_of_englishFractionTuples += returnedLists[i][4] \n",
    "\n",
    "len(List_of_englishFractionTuples) # Number of users you are done storing in this category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1390\n"
     ]
    }
   ],
   "source": [
    "doneUserList = []\n",
    "for eachtuple in List_of_englishFractionTuples:\n",
    "    doneUserList.append(eachtuple[0])\n",
    "print(len(doneUserList))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4641\n",
      "1029\n",
      "3612\n"
     ]
    }
   ],
   "source": [
    "print(len(Allfilenames))\n",
    "removed = 0\n",
    "doneUserList = set(doneUserList)\n",
    "for user in Allfilenames:\n",
    "    if user in doneUserList:\n",
    "        Allfilenames.remove(user)\n",
    "        removed+=1\n",
    "\n",
    "print(removed)\n",
    "print(len(Allfilenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3612\n"
     ]
    }
   ],
   "source": [
    "print(len(Allfilenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1294"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1364-70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1623"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1294+329"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3600/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_multiprocessing():\n",
    "    manager = multiprocessing.Manager()\n",
    "    return_dict = manager.dict()\n",
    "    jobs = []\n",
    "    \n",
    "    start = 3584\n",
    "    jump = 28//28\n",
    "    end = start + 27*jump\n",
    "    \n",
    "    while start <= end :\n",
    "        p = multiprocessing.Process(target=run_language_detection_within_range, args=(start,start+jump,return_dict))\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "        print(start,start+jump)\n",
    "        start = start + jump\n",
    "        \n",
    "    \n",
    "    for proc in jobs:\n",
    "        proc.join()\n",
    "    return return_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3584 3585\n",
      "This is for users in range -  3584 3585\n",
      "3585 3586\n",
      "This is for users in range -  3585 3586\n",
      "3586 3587\n",
      "This is for users in range -  3586 3587\n",
      "3587 3588\n",
      "This is for users in range -  3587 3588\n",
      "3588 3589\n",
      "This is for users in range -  3588 3589\n",
      "3589 3590\n",
      "This is for users in range -  3589 3590\n",
      "3590 3591\n",
      "This is for users in range -  3590 3591\n",
      "3591 3592\n",
      "This is for users in range -  3591 3592\n",
      "3592 3593\n",
      "This is for users in range -  3592 3593\n",
      "3593 3594\n",
      "3594 3595\n",
      "3595 3596\n",
      "3596 3597\n",
      "3597 3598\n",
      "This is for users in range -  3593 3594\n",
      "This is for users in range -  3597 3598\n",
      "3598 3599\n",
      "3599 3600\n",
      "This is for users in range -  3594 3595\n",
      "This is for users in range -  3596 3597\n",
      "This is for users in range -  3598 3599\n",
      "This is for users in range -  3595 3596\n",
      "This is for users in range -  3599 3600\n",
      "3600 3601\n",
      "This is for users in range -  3600 3601\n",
      "3601 3602\n",
      "This is for users in range -  3601 3602\n",
      "3602 3603\n",
      "This is for users in range -  3602 3603\n",
      "3603countdone =  1\n",
      " 3604\n",
      "This is for users in range -  3603 3604\n",
      "3604 3605\n",
      "This is for users in range -  3604 3605\n",
      "3605 3606\n",
      "This is for users in range -  3605 3606\n",
      "3606 3607\n",
      "This is for users in range -  3606 3607\n",
      "3607 3608\n",
      "This is for users in range -  3607 3608\n",
      "3608 3609\n",
      "This is for users in range -  3608 3609\n",
      "3609 3610\n",
      "This is for users in range -  3609 3610\n",
      "3610 3611\n",
      "This is for users in range -  3610 3611\n",
      "This is for users in range -  3611 3612\n",
      "3611 3612\n",
      "User Ethan_Booker number 3595(3595, 3596) , 1302 rows scanned in 40.974241495132446 seconds at 2020-05-20 20:35:45.810218\n",
      "\n",
      "countdone =  0\n",
      "User sed0125 number 3584(3584, 3585) , 2136 rows scanned in 64.37066221237183 seconds at 2020-05-20 20:36:09.005180\n",
      "\n",
      "countdone =  0\n",
      "User Miami4Trump number 3590(3590, 3591) , 2904 rows scanned in 89.02944755554199 seconds at 2020-05-20 20:36:33.724769\n",
      "\n",
      "countdone =  0\n",
      "User koenniki number 3603(3603, 3604) , 3413 rows scanned in 108.58952164649963 seconds at 2020-05-20 20:36:53.477962\n",
      "\n",
      "countdone =  0\n",
      "User RuslanUsachev number 3586(3586, 3587) , 5704 rows scanned in 176.43212223052979 seconds at 2020-05-20 20:38:01.085985\n",
      "\n",
      "countdone =  0\n",
      "User justinjm1 number 3598(3598, 3599) , 10185 rows scanned in 310.7304232120514 seconds at 2020-05-20 20:40:15.566313\n",
      "\n",
      "countdone =  0\n",
      "User 2_slova number 3610(3610, 3611) , 9979 rows scanned in 327.9278771877289 seconds at 2020-05-20 20:40:32.911576\n",
      "\n",
      "countdone =  0\n",
      "User DamonMacWilson number 3591(3591, 3592) , 12258 rows scanned in 374.97289061546326 seconds at 2020-05-20 20:41:19.678664\n",
      "\n",
      "countdone =  0\n",
      "User belka_1D number 3604(3604, 3605) , 14045 rows scanned in 470.19584584236145 seconds at 2020-05-20 20:42:55.097282\n",
      "\n",
      "countdone =  0\n",
      "User RolandTichy number 3600(3600, 3601) , 23920 rows scanned in 694.0684833526611 seconds at 2020-05-20 20:46:38.916999\n",
      "\n",
      "countdone =  0\n",
      "User CityofTwoRivers number 3601(3601, 3602) , 31215 rows scanned in 921.8723094463348 seconds at 2020-05-20 20:50:26.734007\n",
      "\n",
      "countdone =  0\n",
      "User 1948Flatfender number 3608(3608, 3609) , 31177 rows scanned in 946.3974740505219 seconds at 2020-05-20 20:50:51.352075\n",
      "\n",
      "countdone =  0\n",
      "User glaad number 3597(3597, 3598) , 43199 rows scanned in 1281.052610874176 seconds at 2020-05-20 20:56:25.822125\n",
      "\n",
      "countdone =  0\n",
      "User EuromaidanPR number 3585(3585, 3586) , 51579 rows scanned in 1556.6899199485779 seconds at 2020-05-20 21:01:01.334139\n",
      "\n",
      "countdone =  0\n",
      "User ndaktuell number 3592(3592, 3593) , 55200 rows scanned in 1572.796331167221 seconds at 2020-05-20 21:01:17.512753\n",
      "\n",
      "countdone =  0\n",
      "User rickklein number 3589(3589, 3590) , 53940 rows scanned in 1618.4075446128845 seconds at 2020-05-20 21:02:03.092500\n",
      "\n",
      "countdone =  0\n",
      "User sdkstl number 3588(3588, 3589) , 60265 rows scanned in 1782.3046834468842 seconds at 2020-05-20 21:04:46.979245\n",
      "\n",
      "countdone =  0\n",
      "User AmerIndependent number 3606(3606, 3607) , 66280 rows scanned in 1978.9651379585266 seconds at 2020-05-20 21:08:03.891512\n",
      "\n",
      "countdone =  0\n",
      "User Poynter number 3611(3611, 3612) , 66079 rows scanned in 1980.1304233074188 seconds at 2020-05-20 21:08:05.122836\n",
      "\n",
      "countdone =  0\n",
      "User Domyenn number 3593(3593, 3594) , 69301 rows scanned in 2143.6247701644897 seconds at 2020-05-20 21:10:48.385261\n",
      "\n",
      "countdone =  0\n",
      "User Crowdfire number 3587(3587, 3588) , 74216 rows scanned in 2216.2084350585938 seconds at 2020-05-20 21:12:00.872353\n",
      "\n",
      "countdone =  0\n",
      "User Rebeksy number 3607(3607, 3608) , 86750 rows scanned in 2600.2404384613037 seconds at 2020-05-20 21:18:25.180565\n",
      "\n",
      "countdone =  0\n",
      "User darksidedeb number 3599(3599, 3600) , 91478 rows scanned in 2787.345238685608 seconds at 2020-05-20 21:21:32.181412\n",
      "\n",
      "countdone =  0\n",
      "User aldotcom number 3605(3605, 3606) , 159535 rows scanned in 4792.3946759700775 seconds at 2020-05-20 21:54:57.308187\n",
      "\n",
      "countdone =  0\n",
      "User MotherJones number 3609(3609, 3610) , 182547 rows scanned in 5464.053675413132 seconds at 2020-05-20 22:06:09.022267\n",
      "\n",
      "countdone =  0\n",
      "User EW number 3594(3594, 3595) , 204424 rows scanned in 6159.371064901352 seconds at 2020-05-20 22:17:44.206798\n",
      "\n",
      "countdone =  0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "time1 = time.time()\n",
    "ListsReturned = run_multiprocessing()\n",
    "time2 = time.time()\n",
    "file_object = open('TimeTakenforLanguageDetection.txt', 'a')\n",
    "sentence = '\\n\\n********** Total time for last 28 of the remaining 3612 did not mention back users = ' + str((time2-time1)/3600) +'hours. \\n\\n'\n",
    "file_object.write(sentence)\n",
    "print(sentence)\n",
    "file_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the Lists returned as pickle files.\n",
    "\n",
    "import pickle\n",
    "output = open('Famous_Users/User_Data/ListsReturned/MentionedBack_7.pkl', 'wb')\n",
    "pickle.dump(ListsReturned, output)\n",
    "output.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
